{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data process finished.\n",
      "Epoch [1 / 100], step [0 / 1862], loss = 0.58532, lr = 0.000005, elapsed time = 2.69s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8aadeb99173f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\unayd\\Desktop\\EC523_Final_Project\\DnCNN\\cfg.ini\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m     \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-8aadeb99173f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m             \u001b[0mmax_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from pathlib import Path\n",
    "from model import DnCNN\n",
    "import data_generator as dg\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "import cv2\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from skimage.metrics import structural_similarity\n",
    "import glob\n",
    "from BM3D.noise_model import poissonpoissonnoise as nm\n",
    "import datetime\n",
    "\n",
    "\n",
    "# manualSeed = 999\n",
    "# # manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "# print(\"Random Seed: \", manualSeed)\n",
    "# random.seed(manualSeed)\n",
    "# np.random.seed(manualSeed)\n",
    "# torch.manual_seed(manualSeed)\n",
    "\n",
    "def save_model(net: nn.Module, model_save_dir, step, dose_total):\n",
    "    \"\"\"\n",
    "    Save the trained model.\n",
    "\n",
    "    Args:\n",
    "        net: trained model.\n",
    "        model_save_dir: saved model directory.\n",
    "        step: checkpoint.\n",
    "    \"\"\"\n",
    "    model_save_dir = Path(model_save_dir) / \"dose{}\".format(str(int(dose_total)))\n",
    "    if not Path(model_save_dir).exists():\n",
    "        Path.mkdir(model_save_dir)\n",
    "    model_path = Path(model_save_dir) / \"{}.pth\".format(step + 1)\n",
    "\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    print(\"Saved model checkpoints {} into {}\".format(step + 1, model_save_dir))\n",
    "\n",
    "\n",
    "def restore_model(resume_iters, model_save_dir, net: nn.Module, train=True):\n",
    "    \"\"\"\n",
    "    Restore the trained model.\n",
    "\n",
    "    Args:\n",
    "        resume_iters: the iteration to be loaded.\n",
    "        model_save_dir: the directory for saving the model.\n",
    "        net: the model instance to be loaded.\n",
    "        train: if True, then the model is set to training;\n",
    "               else set it to test.\n",
    "\n",
    "    Returns:\n",
    "        net: loaded model instance.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Loading the trained model from step {}\".format(resume_iters))\n",
    "    model_path = Path(model_save_dir) / \"{}.pth\".format(resume_iters)\n",
    "\n",
    "    # Restore the model.\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if train:\n",
    "        net.train()\n",
    "    else:\n",
    "        net.eval()\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "class LossFunc(nn.Module):\n",
    "    def __init__(self, reduction=\"sum\"):\n",
    "        super(LossFunc, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.mse_loss = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, logits, target, eta_min = 2, eta_max = 8, dose = 20, max_values = None, reg = 1e-5):\n",
    "        # Return the average MSE loss.\n",
    "        mse_loss = self.mse_loss(logits, target).div_(2)\n",
    "        dose = dose*100\n",
    "        temp = (eta_max - eta_min)*logits + eta_min\n",
    "        log_loss = torch.mean(torch.log(temp + temp.pow(2) + 0.26) + 1/dose*max_values.pow(2) * torch.div(torch.pow(target - temp.div(max_values)*dose,2), temp + temp.pow(2) + 0.26))\n",
    "        loss = mse_loss + reg*log_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # Define hyper-parameters.\n",
    "    depth = int(config[\"DnCNN\"][\"depth\"])\n",
    "    n_channels = int(config[\"DnCNN\"][\"n_channels\"])\n",
    "    img_channel = int(config[\"DnCNN\"][\"img_channel\"])\n",
    "    kernel_size = int(config[\"DnCNN\"][\"kernel_size\"])\n",
    "    use_bnorm = config.getboolean(\"DnCNN\", \"use_bnorm\")\n",
    "    epochs = int(config[\"DnCNN\"][\"epoch\"])\n",
    "    batch_size = int(config[\"DnCNN\"][\"batch_size\"])\n",
    "    train_data_dir = config[\"DnCNN\"][\"train_data_dir\"]\n",
    "    test_data_dir = config[\"DnCNN\"][\"test_data_dir\"]\n",
    "    eta_min = float(config[\"DnCNN\"][\"eta_min\"])\n",
    "    eta_max = float(config[\"DnCNN\"][\"eta_max\"])\n",
    "    dose = float(config[\"DnCNN\"][\"dose\"])\n",
    "    model_save_dir = config[\"DnCNN\"][\"model_save_dir\"]\n",
    "\n",
    "    # Save logs to txt file.\n",
    "    log_dir = config[\"DnCNN\"][\"log_dir\"]\n",
    "    log_dir = Path(log_dir) / \"dose{}\".format(str(int(dose * 100)))\n",
    "    log_file = log_dir / \"train_result.txt\"\n",
    "\n",
    "    # Define device.\n",
    "    device = torch.device(\"cuda:0\")#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initiate a DnCNN instance.\n",
    "    # Load the model to device and set the model to training.\n",
    "    model = DnCNN(depth=depth, n_channels=n_channels,\n",
    "                  img_channel=img_channel,\n",
    "                  use_bnorm=use_bnorm,\n",
    "                  kernel_size=kernel_size)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Define loss criterion and optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-6)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.2)\n",
    "    criterion = LossFunc(reduction=\"mean\")\n",
    "\n",
    "    # Get a validation test set and corrupt with noise for validation performance.\n",
    "    # For every epoch, use this pre-determined noisy images.\n",
    "    test_file_list = glob.glob(test_data_dir + \"/*.png\")\n",
    "    xs_test = []\n",
    "    # Can't directly convert the xs_test from list to ndarray because some images are 512*512\n",
    "    # while the rest are 256*256.\n",
    "    for i in range(len(test_file_list)):\n",
    "        img = cv2.imread(test_file_list[i], 0)\n",
    "        img = np.array(img, dtype=\"float32\") / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img_noisy, _ = nm(img, eta_min, eta_max, dose, t=100)\n",
    "        xs_test.append((img_noisy, img))\n",
    "\n",
    "    # Train the model.\n",
    "    loss_store = []\n",
    "    epoch_loss_store = []\n",
    "    psnr_store = []\n",
    "    ssim_store = []\n",
    "\n",
    "    psnr_tr_store = []\n",
    "    ssim_tr_store = []\n",
    "    for epoch in range(epochs):\n",
    "        # For each epoch, generate clean augmented patches from the training directory.\n",
    "        # Convert the data from uint8 to float32 then scale them to make it in [0, 1].\n",
    "        # Then make the patches to be of shape [N, C, H, W],\n",
    "        # where N is the batch size, C is the number of color channels.\n",
    "        # H and W are height and width of image patches.\n",
    "        xs = dg.datagenerator(data_dir=train_data_dir)\n",
    "        xs = xs.astype(\"float32\") / 255.0\n",
    "        xs = torch.from_numpy(xs.transpose((0, 3, 1, 2)))\n",
    "\n",
    "        train_set = dg.DenoisingDatatset(xs, eta_min, eta_max, dose)\n",
    "        train_loader = DataLoader(dataset=train_set, num_workers=4,\n",
    "                                  drop_last=True, batch_size=batch_size,\n",
    "                                  shuffle=True)  # TODO: if drop_last=True, the dropping in the\n",
    "                                                 # TODO: data_generator is not necessary?\n",
    "        t_start = timer()\n",
    "        epoch_loss = 0\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            inputs, labels, max_values = data\n",
    "            inputs, labels, max_values = inputs.to(device), labels.to(device), max_values.to(device)\n",
    "            max_values = torch.reshape(max_values,(128,1,1,1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels,eta_min = eta_min, eta_max = eta_max, dose = dose, max_values = max_values, reg = 1e-5 )\n",
    "\n",
    "            loss_store.append(loss.item())\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 100 == 0:\n",
    "                print(\"Epoch [{} / {}], step [{} / {}], loss = {:.5f}, lr = {:.6f}, elapsed time = {:.2f}s\".format(\n",
    "                    epoch + 1, epochs, idx, len(train_loader), loss.item(), *scheduler.get_last_lr(), timer()-t_start))\n",
    "\n",
    "        epoch_loss_store.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # At each epoch validate the result.\n",
    "        model = model.eval()\n",
    "\n",
    "        val_psnr = []\n",
    "        val_ssim = []\n",
    "        with torch.no_grad():\n",
    "            for idx, test_data in enumerate(xs_test):\n",
    "                inputs, labels = test_data\n",
    "                inputs = np.expand_dims(inputs, axis=0)\n",
    "                inputs = torch.from_numpy(inputs).to(device)\n",
    "                labels = labels.squeeze()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze().cpu().detach().numpy()\n",
    "\n",
    "                val_psnr.append(peak_signal_noise_ratio(labels, outputs))\n",
    "                val_ssim.append(structural_similarity(outputs, labels))\n",
    "\n",
    "        psnr_store.append(sum(val_psnr) / len(val_psnr))\n",
    "        ssim_store.append(sum(val_ssim) / len(val_ssim))\n",
    "\n",
    "        print(\"Validation on test set: epoch [{} / {}], aver PSNR = {:.2f}, aver SSIM = {:.4f}\".format(\n",
    "            epoch + 1, epochs, psnr_store[-1], ssim_store[-1]))\n",
    "\n",
    "        # Set model to train mode again.\n",
    "        model = model.train()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save model\n",
    "        save_model(model, model_save_dir, epoch, dose * 100)\n",
    "\n",
    "        # Save the loss and validation PSNR, SSIM.\n",
    "\n",
    "        if not log_dir.exists():\n",
    "            Path.mkdir(log_dir)\n",
    "        with open(log_file, \"a+\") as fh:\n",
    "            # fh.write(\"{} Epoch [{} / {}], loss = {:.6f}, train PSNR = {:.2f}dB, train SSIM = {:.4f}, \"\n",
    "            #          \"validation PSNR = {:.2f}dB, validation SSIM = {:.4f}\".format(\n",
    "            #          datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:\"),\n",
    "            #          epoch + 1, epochs, epoch_loss_store[-1],\n",
    "            #          psnr_tr_store[-1], ssim_tr_store[-1],\n",
    "            #          psnr_store[-1], ssim_store[-1]))\n",
    "            fh.write(\"{} Epoch [{} / {}], loss = {:.6f}, \"\n",
    "                     \"validation PSNR = {:.2f}dB, validation SSIM = {:.4f}\\n\".format(\n",
    "                     datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S:\"),\n",
    "                     epoch + 1, epochs, epoch_loss_store[-1],\n",
    "                     psnr_store[-1], ssim_store[-1]))\n",
    "\n",
    "        # np.savetxt(log_file, np.hstack((epoch + 1, epoch_loss_store[-1], psnr_store[-1], ssim_store[-1])), fmt=\"%.6f\", delimiter=\",  \")\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(loss_store[-len(train_loader):])\n",
    "        ax.set_title(\"Last 1862 losses\")\n",
    "        ax.set_xlabel(\"iteration\")\n",
    "        fig.show()\n",
    "\n",
    "    # print(\"Continue\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    config.read(r\"C:\\Users\\unayd\\Desktop\\EC523_Final_Project\\DnCNN\\cfg.ini\")\n",
    "\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
